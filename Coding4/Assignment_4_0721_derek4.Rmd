---
title: "Coding4 PSL Fall 2020"
author: "Derek Chapman (derek4), Zeed Jarrah (Ùè∞£zjarrah2)"
date: "10/28/2020"
output: 
  html_document: 
    highlight: kate
    theme: yeti
---

```{r, setup, message=FALSE}
library(mclust)
library(kableExtra) #for table at end
options(digits = 7)
```

## Our EM Algorithm

#### E-Step

```{r, e-step}
Estep <- function (data, G, para) {
  prob1_vec = para$prob[1]*densityMvNorm(faithful, para$mean[,1], para$Sigma)/
    (para$prob[1]*densityMvNorm(faithful, para$mean[,1], para$Sigma) + 
     para$prob[2]*densityMvNorm(faithful, para$mean[,2], para$Sigma))

  # Return the n-by -G probability matrix
  return (cbind(prob1_vec, 1-prob1_vec))
}
```

#### M-Step

```{r, m-step}
Mstep <- function (data, G, para, post.prob) {
  prob = apply(post.prob, 2, mean)
  
  rnk_Xn1 = post.prob[,1]*faithful
  mean1   = apply(rnk_Xn1, 2,sum)/apply(post.prob,2,sum)[1]
  rnk_Xn2 = post.prob[,2]*faithful
  mean2   = apply(rnk_Xn2, 2,sum)/apply(post.prob,2,sum)[2]
  updated_mean = cbind(mean1, mean2)
  
  mat_temp1 = matrix(rep(0, 4), nrow=2)
  
  for (i in 1:nrow(faithful)) {
    x1 = post.prob[i,1] * (t(faithful[i,]-mean1) %*% t(t(faithful[i,]-mean1)))
    x2 = post.prob[i,2] * (t(faithful[i,]-mean2) %*% t(t(faithful[i,]-mean2)))
    
    mat_temp1 = mat_temp1 + x1 + x2
  }
  
  covar_matrix = mat_temp1/nrow(faithful)

  # Return the updated parameters
  return(list(prob = prob, mean = updated_mean, Sigma = covar_matrix))
}
```

#### EM Cycle

```{r, em-loop}
myEM <- function (data, itmax, G, para){
  for(t in 1: itmax ) {
    post.prob = Estep (data , G , para )
    para = Mstep (data, G, para, post.prob)
  }
  return(para)
}
```

#### Our Multi-variate Density Function

```{r our-dmvnorm}
densityMvNorm <- function (x, mean, sigma, log = FALSE) 
{
  mean = as.vector(mean)

  if (missing(sigma)) sigma = diag(ncol(x))
  
  if (NCOL(x) != NCOL(sigma)) {
    stop("x and sigma have non-conforming size")
  }
  if (!isSymmetric(sigma, tol = sqrt(.Machine$double.eps), 
                   check.attributes = FALSE)) {
    stop("sigma must be a symmetric matrix")
  }
  if (length(mean) != NROW(sigma)) {
    print(NCOL(mean))
    print(NROW(sigma))
    stop("mean and sigma have non-conforming size")
  }
  
  ## invert matrix before hand so only do this once
  prec = solve(sigma)
  distval = mahalanobis(x, center = mean, cov = prec, inverted = TRUE)
  logdet = sum(log(eigen(sigma, symmetric = TRUE, only.values = TRUE)$values))
  logretval = -(ncol(x) * log(2 * pi) + logdet + distval)/2
  
  if (log) 
    return(unname(logretval))
  else 
    return(exp(logretval))
}
```

Based on [csgillespie's custom dmvnorm function](https://github.com/csgillespie/abcpmcmc/blob/master/R/dmvnorm.R)

With significant help from the actual mclust function found at: https://rdrr.io/cran/mclust/src/R/util.R

## Testing Our Functions

#### Setup

```{r, setup-test}
set.seed(9618)

# initialize parameters
n = nrow(faithful)
Z = matrix (0 , n , 2)
Z[sample(1:n, 120), 1] = 1
Z[, 2] = 1 - Z[, 1]

ini0 <- mstep( modelName ="EEE", faithful , Z )$parameters
para0 <- list ( prob = ini0$pro, 
                mean = ini0$mean,
                Sigma = ini0$variance$Sigma )
```

#### Results from our function and mclust

```{r get-results}
# Output from our EM alg
my_out = myEM(data = faithful, 
              itmax = 10, G= 2, para = para0)

# Output from mclust
Rout = em(modelName = "EEE", 
          data = faithful,
          control = emControl(eps = 0, tol = 0, itmax = 10),
          parameters = ini0)$parameters
```

#### Compare Results

```{r, results}
probs = cbind(my_out$prob, Rout$pro)
colnames(probs) = c("Our EM", "mclust")
rownames(probs) = c("Cluster 1", "Cluster 2")

knitr::kable(probs, format = 'html', align = 'c', caption = "Results Comparison") %>%
  kable_styling(bootstrap_options = "striped",
                full_width = F, font_size = 18,
                position = 'center')
```



## Derivatives (Parts and steps of the problem)

### Complete Data Log-Likelihood

We would like to calculate the true complete log-likelihood but that is not possible since we do not have the true labels to make a supervised learning model (such as in Naive Bayes).  Instead we use a latent *indicator* variable $z_n$ that typically begins as a random assignment and then is refined throughout the EM steps.
$$
\ell_c(\theta) = \log \prod_n p(z_n | \pi) p(x_n|z_n, \theta)
$$

Notice that we are essentially creating a generative model since we are looking at the probability that data point $x_n$ is created based on the assigned label and current parameters $\theta$:  $p(x_n | z_n = k, \theta)$

We assume a Gaussian distribution for our clusters: $N(x_n|\mu_k, \Sigma)$

### Incomplete Log-Likelihood

The incomplete data log likelihood is difficult to calculate directly due to the $log$ being sandwiched between two summations.
$$
\begin{align}
\ell(\theta) &= \sum_n \log p(x_n | \theta) \\
&= \sum_n \log \sum_{z_n = 1}^K \pi(z_n) N(x_n | z_n, \mu(z_n), \Sigma)
\end{align}
$$

Again we are using this as a generative model since we now directly look at the conditional probability that our data point was created based on our current parameters: $p(x_n | \theta)$ which is then expanded out into a joint probability of the data point and assigned label conditioned on the current parameters: $p(x_n, z_n | \theta)$



### Parameters

Here our $\theta$ represents the parameters of our model.  Our label means/centers ($\mu$), our label mixing weights ($\pi$), and our variance ($\Sigma$).  Due to the type of our model we have a means and mixing weights for each label but use a common co-variance matrix.  In this case it will be a 2x2 matrix since we have 2 labels/clusters.

Our initial **mixing weights** are a simple ratio:
$$
\pi_k = \frac{1}{N}\sum_{n} x_k
$$

We can see this is the case in the starting parameters given from the `mclust` package:

```{r, derivations-parms-mixing}
ini0  = mstep(modelName = "EEE", faithful, Z)$parameters
ini0$pro[1]
sum(Z[, 1])/n
```



The initial **centers** can either be created by selecting a random datapoint that was giving a starting assignment to that label or by taking the mean from every assigned datapoint.

The `mclust` package appears to use the mean of the starting assignments for each label.  Again we can check that this is the case using the starting assignments for the first cluster as an example:

```{r, derivatives-parms-means}
t1 = mean(faithful[Z[,1] == 1, 1])
t2 = mean(faithful[Z[,1] == 1, 2])
c(t1, t2)
ini0$mean[, 1]
```



### E-Step

During the E-step we recalculate the latent indicator labels given the current parameters.  Since we are using a soft encoding method our individual data points will get a probability that they are in each of the labels (partial membership).  This is different from a hard coded method such as k-means where a data point can only be part of a single class at a time.  Because of this we need to calculate a much more intricate density function since we now have a single observation that could be part of two (or more) different distributions, we need calculate a multi-variate density function.

$$
p(z_n = k | x_n, \theta) = \frac{\pi_k N(x_n|u_k, \Sigma)}
{\sum_j \pi_j N(x_n | \mu_j, \Sigma)}
$$

This probability is referred to as the **responsibility** that data point $n$ was generated by label/cluster $k$.  We look at the probability distribution function for an individual class label and divide this by the total probability from all of the distributions to get our probability for an individual class.



### M-Step

Then during the M-Step we recalculate our base parameters each time based on the current estimated latent variable groupings.  We start with our generic objective function trying to maximize the log-likelihood given the estimated parameters.

$$
\begin{align}
&= \sum_n \log \space p(x_n, z_n | \theta) \\
&= \sum_n log \prod \pi_k N(x_n | u_k, \Sigma)^{I(z_n = k)} \\
&= \sum_n \sum_k I(z_n = k) \space \log[\pi_k N(x_n | \mu_k, \Sigma)] \\
&= \sum_n \sum_k r_{nk} \log \pi_k + \sum_n \sum_k r_{nk} \log[N(x_n | \mu_k, \Sigma)]
\end{align}
$$



**M-Step Mixing Weights**

During the M-step our mixing weights are no longer a simple ratio but rather a weighted ratio that uses the responsibility  ($r_{nk})$of a cluster.  That is we look at how likely it is that a cluster generated a specific point.  These can be viewed as the posterior probabilities since we have now 'observed' x and our original mixing weights as the prior probabilities.
$$
\pi_k = \frac{1}{N}\sum_{n} r_{nk}
$$

**M-Step Means/Centers**

Our new means also make use of the responsibility of a cluster for an individual datapoint.
$$
\mu_k = \frac{\sum_n r_{nk} x_n}{\sum_n r_{nk}}
$$
**M-Step Covariance Matrix**
$$
\Sigma = \frac{1}{N} \sum_n r_{n1} (x_n - \mu_1)(x_n - \mu_1)^t + r_{n2} (x_n - \mu_2)(x_n - \mu_2)^t
$$


### Equations

All math equations typeset using [LaTeX](https://www.latex-project.org/about/) created in [Typora](https://typora.io/) then rendered in RStudio via the Rmarkdown document.