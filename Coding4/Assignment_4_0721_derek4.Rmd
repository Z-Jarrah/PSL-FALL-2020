---
title: "Coding4 PSL Fall 2020"
author: "Derek Chapman (derek4), Zeed Jarrah (Ùè∞£zjarrah2)"
date: "10/28/2020"
output: 
  html_document: 
    highlight: kate
    theme: yeti
---

```{r, setup}
library(mclust)
options(digits = 7)
```



## Derivatives (Parts and steps of the problem)



Derivatives with regards to pi, mu, and Sigma: Yes
-- State what the parameters, i.e., elements of theta, are. Introduce each symbol in your theta, which should contain the (pi 1 + 2) mixing weights, mean vectors, and a shared 2-by-2 covariance matrix.

and finally the formulae for the MLE for pi, mu, and Sigma: Yes



### Incomplete Log-Likelihood

We would like to calculate the incomplete data log likelihood since we do not have the true labels to make a supervised learning model (such as in Naive Bayes).  However this is difficult due to the log being sandwiched between two summations.
$$
\begin{align}
\ell(\theta) &= \sum_n \log p(x_n | \theta) \\
&= \sum_n \log \sum_{z_n = 1}^K \pi(z_n) N(x_n | z_n, \mu(z_n), \Sigma)
\end{align}
$$


### Adjusted Log-Likelihood

Because of this we calculate our likelihood using a latent *indicator* variable $z_n$ that typically begins as a random assignment and then is refined throughout the EM steps.
$$
\ell_c(\theta) = \log \prod_n p(z_n | \pi) p(x_n|z_n, \theta)
$$

### Parameters

Here our $\theta$ represents the parameters of our model.  Our label means/centers ($\mu$), our label mixing weights ($\pi$), and our variance ($\Sigma$).  Due to the type of our model we have a means and mixing weights for each label but use a common co-variance matrix.  In this case it will be a 2x2 matrix since we have 2 labels/clusters.

Our initial **mixing weights** are a simple ratio:
$$
\pi_k = \frac{1}{N}\sum_{n} x_k
$$

We can see this is the case in the starting parameters given from the `mclust` package:

```{r, derivations-parms-mixing}
ini0  = mstep(modelName = "EEE", faithful, Z)$parameters
ini0$pro
sum(Z[, 1])/n
```



The initial **centers** can be either created by selecting a random datapoint that was giving a starting assignment to that label or by taking the mean from every included datapoint.

The `mclust` package appears to use the mean of the starting assignments for each label.  Again we can check that this is the case using the starting assignments for the first cluster as an example:

```{r, derivatives-parms-means}
t1 = mean(faithful[Z[,1] == 1, 1])
t2 = mean(faithful[Z[,1] == 1, 2])
c(t1, t2)
ini0$mean[, 1]
```



### E-Step

During the E-step we recalculate the latent indicator labels given the current parameters.  Since we are using a soft encoding method our individual data points will get a probability that they are in each of the labels (partial membership).  This is different from a hard coded method such as k-means where a data point can only be part of a single class at a time.

$$
p(z_n = k | x_n, \theta) = \frac{\pi_k N(x_n|u_k, \Sigma)}
{\sum_j \pi_j N(x_n | \mu_j, \Sigma)}
$$
This probability is referred to as the **responsibility** that data point $n$ was generated by label/cluster $k$.  We look at the probability distribution function for an individual class label and divide this by the total probability from all of the distributions to get our probability for an individual class.



### M-Step

Then during the M-Step we recalculate our base parameters each time based on the current estimated latent variable groupings.  We start with our generic objective function trying to maximize the log-likelihood given the estimated parameters.
$$
\begin{align}
&= \sum_n \log \space p(x_n, z_n | \theta) \\
&= \sum_n log \prod \pi_k N(x_n | u_k, \Sigma)^{I(z_n = k)} \\
&= \sum_n \sum_k I(z_n = k) \space \log[\pi_k N(x_n | \mu_k, \Sigma)] \\
&= \sum_n \sum_k r_{nk} \log \pi_k + \sum_n \sum_k r_{nk} \log[N(x_n | \mu_k, \Sigma)]
\end{align}
$$



**M-Step Mixing Weights**

During the M-step our mixing weights are no longer a simple ratio but rather a weighted ratio that uses the responsibility  ($r_{nk})$of a cluster.  That is we look at how likely it is that a cluster generated a specific point.  These can be viewed as the posterior probabilities since we have now 'observed' x and our original mixing weights as the prior probabilities.
$$
\pi_k = \frac{1}{N}\sum_{n} r_{nk}
$$

**M-Step Means/Centers**

Our new means also make use of the responsibility of a cluster for an individual datapoint.
$$
\mu_k = \frac{\sum_n r_{nk} x_n}{\sum_n r_{nk}}
$$
**M-Step Covariance Matrix**
$$
\Sigma = \frac{1}{N} \sum_n \pi_1 (x_n - \mu_1) + \pi_2 (x_n - \mu_2)
$$
